<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Vanishing gradient problem - Wikipedia</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Vanishing_gradient_problem","wgTitle":"Vanishing gradient problem","wgCurRevisionId":863125122,"wgRevisionId":863125122,"wgArticleId":43502368,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Articles lacking reliable references from December 2017","All articles lacking reliable references","Wikipedia articles needing clarification from October 2018","All articles with unsourced statements","Articles with unsourced statements from June 2017","Machine learning","Artificial neural networks"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Vanishing_gradient_problem","wgRelevantArticleId":43502368,"wgRequestId":"W9MZPApAAEMAAF0D-nMAAACD","wgCSPNonce":false,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgWikiEditorEnabledModules":[],"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsShouldSendModuleToUser":true,"wgPopupsConflictsWithNavPopupGadget":false,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en","usePageImages":true,"usePageDescriptions":true},"wgMFExpandAllSectionsUserOption":true,"wgMFEnableFontChanger":true,"wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgWikibaseItemId":"Q18358230","wgScoreNoteLanguages":{"arabic":"العربية","catalan":"català","deutsch":"Deutsch","english":"English","espanol":"español","italiano":"italiano","nederlands":"Nederlands","norsk":"norsk","portugues":"português","suomi":"suomi","svenska":"svenska","vlaams":"West-Vlams"},"wgScoreDefaultNoteLanguage":"nederlands","wgCentralAuthMobileDomain":false,"wgCodeMirrorEnabled":true,"wgVisualEditorToolbarScrollOffset":0,"wgVisualEditorUnsupportedEditParams":["undo","undoafter","veswitched"],"wgEditSubmitButtonLabelPublish":true});mw.loader.state({"ext.gadget.charinsert-styles":"ready","ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.cite.styles":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.toc.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","ext.3d.styles":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready"});mw.loader.implement("user.tokens@0tffind",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});RLPAGEMODULES=["ext.cite.a11y","site","mediawiki.page.startup","mediawiki.user","mediawiki.page.ready","mediawiki.toc","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging.subscriber","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.compactlinks","ext.uls.interface","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"];mw.loader.load(RLPAGEMODULES);});</script>
<link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=ext.3d.styles%7Cext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.skinning.interface%7Cmediawiki.toc.styles%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?debug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=ext.gadget.charinsert-styles&amp;only=styles&amp;skin=vector"/>
<link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.33.0-wmf.1"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/1200px-Kernel_Machine.svg.png"/>
<link rel="alternate" href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Vanishing_gradient_problem"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Vanishing_gradient_problem&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="/w/index.php?title=Vanishing_gradient_problem&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<!--[if lt IE 9]><script src="/w/load.php?debug=false&amp;lang=en&amp;modules=html5shiv&amp;only=scripts&amp;skin=vector&amp;sync=1"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-Vanishing_gradient_problem rootpage-Vanishing_gradient_problem skin-vector action-view">		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<div id="content" class="mw-body" role="main">
			<a id="top"></a>
			<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div><div class="mw-indicators mw-body-content">
</div>
<h1 id="firstHeading" class="firstHeading" lang="en">Vanishing gradient problem</h1>			<div id="bodyContent" class="mw-body-content">
				<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>				<div id="contentSub"></div>
				<div id="jump-to-nav"></div>				<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
				<a class="mw-jump-link" href="#p-search">Jump to search</a>
				<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a> and<br /><a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td style="padding:0.2em 0 0.4em;padding:0.25em 0.25em 0.75em;"><a href="/wiki/File:Kernel_Machine.svg" class="image"><img alt="Kernel Machine.svg" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/220px-Kernel_Machine.svg.png" width="220" height="100" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/330px-Kernel_Machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/440px-Kernel_Machine.svg.png 2x" data-file-width="512" data-file-height="233" /></a></td></tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>
<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><div style="padding:0.1em 0;line-height:1.2em;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br /><style data-mw-deduplicate="TemplateStyles:r865336399">.mw-parser-output .nobold{font-weight:normal}</style><span class="nobold"><span style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b>&#160;&#8226;&#32;<b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>
<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>
<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>
<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Neural networks</a></li>
<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a href="/wiki/Support_vector_machine" title="Support vector machine">Support vector machine (SVM)</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a href="/wiki/CURE_data_clustering_algorithm" class="mw-redirect" title="CURE data clustering algorithm">CURE</a></li>
<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization (EM)</a></li>
<li><br /><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a href="/wiki/Mean-shift" class="mw-redirect" title="Mean-shift">Mean-shift</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="/wiki/Canonical_correlation_analysis" class="mw-redirect" title="Canonical correlation analysis">CCA</a></li>
<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>
<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>
<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>
<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/K-nearest_neighbors_classification" class="mw-redirect" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>
<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Neural nets</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a>
<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a>)</li></ul></li>
<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>
<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li></ul></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Bias-variance_dilemma" class="mw-redirect" title="Bias-variance dilemma">Bias-variance dilemma</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine-learning venues</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NIPS</a></li>
<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>
<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>
<li><a rel="nofollow" class="external text" href="https://arxiv.org/list/cs.LG/recent">ArXiv:cs.LG</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Related articles</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/List_of_datasets_for_machine-learning_research" class="mw-redirect" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>
<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul>
</div></div></div></td>
</tr><tr><td class="plainlist" style="padding:0.3em 0.4em 0.3em;font-weight:bold;border-top: 1px solid #aaa; border-bottom: 1px solid #aaa;border-top:1px solid #aaa;border-bottom:1px solid #aaa;">
<ul><li><a href="/wiki/File:Portal-puzzle.svg" class="image"><img alt="Portal-puzzle.svg" src="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/16px-Portal-puzzle.svg.png" width="16" height="14" class="noviewer" srcset="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/24px-Portal-puzzle.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/32px-Portal-puzzle.svg.png 2x" data-file-width="32" data-file-height="28" /></a> <a href="/wiki/Portal:Machine_learning" title="Portal:Machine learning">Machine learning&#32;portal</a></li></ul></td></tr><tr><td style="text-align:right;font-size:115%;padding-top: 0.6em;"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<p><a href="/wiki/Machine_learning" title="Machine learning">In machine learning</a>, the <b>vanishing gradient problem</b> is a difficulty found in training <a href="/wiki/Artificial_neural_network" title="Artificial neural network">artificial neural networks</a> with <a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">gradient-based learning methods</a> and <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a>. In such methods, each of the neural network's weights receives an update proportional to the <a href="/wiki/Partial_derivative" title="Partial derivative">partial derivative</a> of the <a href="/wiki/Error_function" title="Error function">error function</a> with respect to the current weight in each iteration of training. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training. As one example of the problem cause, traditional <a href="/wiki/Activation_function" title="Activation function">activation functions</a> such as the <a href="/wiki/Hyperbolic_tangent" class="mw-redirect" title="Hyperbolic tangent">hyperbolic tangent</a> function have gradients in the range <span class="texhtml">(0, 1)</span>, and backpropagation computes gradients by the <a href="/wiki/Chain_rule" title="Chain rule">chain rule</a>. This has the effect of multiplying <span class="texhtml mvar" style="font-style:italic;">n</span> of these small numbers to compute gradients of the "front" layers in an <span class="texhtml mvar" style="font-style:italic;">n</span>-layer network, meaning that the gradient (error signal) decreases exponentially with <span class="texhtml mvar" style="font-style:italic;">n</span> while the front layers train very slowly.
</p><p>Back-propagation allowed researchers to train <a href="/wiki/Supervised_learning" title="Supervised learning">supervised</a> deep artificial neural networks from scratch, initially with little success. <a href="/wiki/Sepp_Hochreiter" title="Sepp Hochreiter">Hochreiter</a>'s diploma thesis of 1991<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">&#91;1&#93;</a></sup><sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup> formally identified the reason for this failure in the "vanishing gradient problem", which not only affects <a href="/wiki/Deep_learning" title="Deep learning">many-layered</a> <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feedforward networks</a>,<sup id="cite_ref-3" class="reference"><a href="#cite_note-3">&#91;3&#93;</a></sup> but also <a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent networks</a>.<sup id="cite_ref-4" class="reference"><a href="#cite_note-4">&#91;4&#93;</a></sup> The latter are trained by unfolding them into very deep feedforward networks, where a new layer is created for each time step of an input sequence processed by the network.
</p><p>When activation functions are used whose derivatives can take on larger values, one risks encountering the related exploding gradient problem.
</p>
<div class="toclimit-3"><div id="toc" class="toc"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2>Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Solutions"><span class="tocnumber">1</span> <span class="toctext">Solutions</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Multi-level_hierarchy"><span class="tocnumber">1.1</span> <span class="toctext">Multi-level hierarchy</span></a>
<ul>
<li class="toclevel-3 tocsection-3"><a href="#Related_approach"><span class="tocnumber">1.1.1</span> <span class="toctext">Related approach</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-4"><a href="#Long_short-term_memory"><span class="tocnumber">1.2</span> <span class="toctext">Long short-term memory</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="#Faster_hardware"><span class="tocnumber">1.3</span> <span class="toctext">Faster hardware</span></a></li>
<li class="toclevel-2 tocsection-6"><a href="#Residual_networks"><span class="tocnumber">1.4</span> <span class="toctext">Residual networks</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="#Other_activation_functions"><span class="tocnumber">1.5</span> <span class="toctext">Other activation functions</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#Other"><span class="tocnumber">1.6</span> <span class="toctext">Other</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-9"><a href="#See_also"><span class="tocnumber">2</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-10"><a href="#References"><span class="tocnumber">3</span> <span class="toctext">References</span></a></li>
</ul>
</div>
</div>
<h2><span class="mw-headline" id="Solutions">Solutions</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Vanishing_gradient_problem&amp;action=edit&amp;section=1" title="Edit section: Solutions">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<table class="plainlinks metadata ambox ambox-content ambox-Primary_sources" role="presentation"><tbody><tr><td class="mbox-image"><div style="width:52px"><a href="/wiki/File:Question_book-new.svg" class="image"><img alt="Question book-new.svg" src="//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/50px-Question_book-new.svg.png" width="50" height="39" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/75px-Question_book-new.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/100px-Question_book-new.svg.png 2x" data-file-width="512" data-file-height="399" /></a></div></td><td class="mbox-text"><div class="mbox-text-span">This section <b>needs additional <a href="/wiki/Wikipedia:Scientific_citation_guidelines" title="Wikipedia:Scientific citation guidelines">citations</a> to <a href="/wiki/Wikipedia:Reliable_sources#Scholarship" class="mw-redirect" title="Wikipedia:Reliable sources">secondary or tertiary sources</a></b><span class="hide-when-compact"> such as review articles, monographs, or textbooks.  Please add such references to provide context and establish the relevance of any <a href="/wiki/Wikipedia:No_original_research#Primary,_secondary_and_tertiary_sources" title="Wikipedia:No original research">primary research articles</a> cited. Unsourced or poorly sourced material may be challenged and removed.</span>  <small><i>(December 2017)</i></small><small class="hide-when-compact"><i> (<a href="/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>
<table class="plainlinks metadata ambox ambox-content ambox-unreliable_sources" role="presentation"><tbody><tr><td class="mbox-image"><div style="width:52px"><a href="/wiki/File:Text_document_with_red_question_mark.svg" class="image"><img alt="Text document with red question mark.svg" src="//upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Text_document_with_red_question_mark.svg/40px-Text_document_with_red_question_mark.svg.png" width="40" height="40" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Text_document_with_red_question_mark.svg/60px-Text_document_with_red_question_mark.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Text_document_with_red_question_mark.svg/80px-Text_document_with_red_question_mark.svg.png 2x" data-file-width="48" data-file-height="48" /></a></div></td><td class="mbox-text"><div class="mbox-text-span">Some of this  section 's <a href="/wiki/Wikipedia:Citing_sources" title="Wikipedia:Citing sources">listed sources</a> <b>may not be <a href="/wiki/Wikipedia:Identifying_reliable_sources" title="Wikipedia:Identifying reliable sources">reliable</a></b>.<span class="hide-when-compact"> Please help this article by looking for better, more reliable sources. Unreliable citations may be challenged or deleted.</span>  <small><i>(December 2017)</i></small><small class="hide-when-compact"><i> (<a href="/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>
<h3><span class="mw-headline" id="Multi-level_hierarchy">Multi-level hierarchy</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Vanishing_gradient_problem&amp;action=edit&amp;section=2" title="Edit section: Multi-level hierarchy">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>To overcome this problem, several methods were proposed. One is <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Jürgen Schmidhuber</a>'s  multi-level hierarchy of networks (1992) pre-trained one level at a time through <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a>, fine-tuned through <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a>.<sup id="cite_ref-SCHMID1992_5-0" class="reference"><a href="#cite_note-SCHMID1992-5">&#91;5&#93;</a></sup> Here each level learns a compressed representation of the observations that is fed to the next level.
</p>
<h4><span class="mw-headline" id="Related_approach">Related approach</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Vanishing_gradient_problem&amp;action=edit&amp;section=3" title="Edit section: Related approach">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Similar ideas have been used in feed-forward neural network for unsupervised pre-training to structure a neural network, making it first learn generally useful <a href="/wiki/Feature_detection_(nervous_system)" title="Feature detection (nervous system)">feature detectors</a>. Then the network is trained further by supervised <a href="/wiki/Back-propagation" class="mw-redirect" title="Back-propagation">back-propagation</a> to classify labeled data. The <a href="/wiki/Deep_belief_network" title="Deep belief network">deep belief network</a> model by Hinton et al. (2006) involves learning the distribution of a high level representation using successive layers of binary or real-valued  <a href="/wiki/Latent_variable" title="Latent variable">latent variables</a>. It uses a <a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">restricted Boltzmann machine</a> to model each new layer of higher level features. Each new layer guarantees an increase on the <a href="/wiki/Lower_bound" class="mw-redirect" title="Lower bound">lower-bound</a> of the <a href="/wiki/Log_likelihood" class="mw-redirect" title="Log likelihood">log likelihood</a> of the data, thus improving the model, if trained properly. Once sufficiently many layers have been learned the deep architecture may be used as a <a href="/wiki/Generative_model" title="Generative model">generative model</a> by reproducing the data when sampling down the model (an "ancestral pass") from the top level feature activations.<sup id="cite_ref-hinton2006_6-0" class="reference"><a href="#cite_note-hinton2006-6">&#91;6&#93;</a></sup>
Hinton reports that his models are effective feature extractors over high-dimensional, structured data.<sup id="cite_ref-7" class="reference"><a href="#cite_note-7">&#91;7&#93;</a></sup> This work played a key role in reintroducing interest in deep neural network research and consequently led to the developments of <a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a>, although deep belief network is no longer the primary deep learning technique.
</p>
<h3><span class="mw-headline" id="Long_short-term_memory">Long short-term memory</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Vanishing_gradient_problem&amp;action=edit&amp;section=4" title="Edit section: Long short-term memory">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Long_short-term_memory" title="Long short-term memory">Long short-term memory</a></div>
<p>Another technique particularly used for <a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent neural networks</a> is the <a href="/wiki/Long_short-term_memory" title="Long short-term memory">long short-term memory</a> (LSTM) network of 1997 by <a href="/wiki/Sepp_Hochreiter" title="Sepp Hochreiter">Hochreiter</a> &amp; <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Schmidhuber</a>.<sup id="cite_ref-lstm_8-0" class="reference"><a href="#cite_note-lstm-8">&#91;8&#93;</a></sup> In 2009, deep multidimensional LSTM networks demonstrated the power of deep learning with many nonlinear layers, by winning three <a href="/wiki/ICDAR" class="mw-redirect" title="ICDAR">ICDAR</a> 2009 competitions in connected <a href="/wiki/Handwriting_recognition" title="Handwriting recognition">handwriting recognition</a>, without any prior knowledge about the three different languages to be learned.<sup id="cite_ref-9" class="reference"><a href="#cite_note-9">&#91;9&#93;</a></sup><sup id="cite_ref-10" class="reference"><a href="#cite_note-10">&#91;10&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Faster_hardware">Faster hardware</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Vanishing_gradient_problem&amp;action=edit&amp;section=5" title="Edit section: Faster hardware">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Hardware advances have meant that from 1991 to 2015, computer power (especially as delivered by <a href="/wiki/General-purpose_computing_on_graphics_processing_units" title="General-purpose computing on graphics processing units">GPUs</a>) has increased around a million-fold, making standard backpropagation feasible for networks several layers deeper than when the vanishing gradient problem was recognized. Schmidhuber notes that this "is basically what is winning many of the image recognition competitions now", but that it "does not really
overcome the problem in a fundamental way"<sup id="cite_ref-11" class="reference"><a href="#cite_note-11">&#91;11&#93;</a></sup> since the original models tackling the vanishing gradient problem by Hinton et al. (2006) were trained in a <a href="/wiki/Xeon" title="Xeon">Xeon processor</a>, not GPUs.<sup id="cite_ref-hinton2006_6-1" class="reference"><a href="#cite_note-hinton2006-6">&#91;6&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Residual_networks">Residual networks</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Vanishing_gradient_problem&amp;action=edit&amp;section=6" title="Edit section: Residual networks">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>One of the newest and most effective ways to resolve the vanishing gradient problem is with residual neural networks, ResNets,<sup id="cite_ref-12" class="reference"><a href="#cite_note-12">&#91;12&#93;</a></sup> not to be confused with recurrent neural networks.<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">&#91;13&#93;</a></sup> It was noted prior to ResNets that a deeper network would actually have higher <i>training</i> error than the shallow network. This intuitively can be understood as data disappearing through too many layers of the network, meaning output from a shallow layer was diminished through the greater number of layers in the deeper network, yielding a worse result. Going with this intuitive hypothesis, Microsoft research found that splitting a deep network into three layer chunks and passing the input into each chunk straight through to the next chunk, along with the residual-output of the chunk minus the input to the chunk that is reintroduced, helped eliminate much of this disappearing signal problem.<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="needs to be explained carefully (October 2018)">clarification needed</span></a></i>&#93;</sup> No extra parameters or changes to the learning algorithm were needed. ResNets<sup id="cite_ref-14" class="reference"><a href="#cite_note-14">&#91;14&#93;</a></sup> yielded lower training error (and test error) than their shallower counterparts simply by reintroducing outputs from shallower layers in the network to compensate for the vanishing data.<sup id="cite_ref-15" class="reference"><a href="#cite_note-15">&#91;15&#93;</a></sup>
</p><p>Note that ResNets are an ensemble of relatively shallow Nets and do not resolve the vanishing gradient problem by preserving gradient flow throughout the entire depth of the network – rather, they avoid the problem simply by constructing ensembles of many short networks together. (Ensemble by Construction<sup id="cite_ref-16" class="reference"><a href="#cite_note-16">&#91;16&#93;</a></sup>)
</p>
<h3><span class="mw-headline" id="Other_activation_functions">Other activation functions</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Vanishing_gradient_problem&amp;action=edit&amp;section=7" title="Edit section: Other activation functions">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/Rectifier_(neural_networks)" title="Rectifier (neural networks)">Rectifiers</a> such as ReLU suffer less from the vanishing gradient problem, because they only saturate in one direction.<sup id="cite_ref-17" class="reference"><a href="#cite_note-17">&#91;17&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Other">Other</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Vanishing_gradient_problem&amp;action=edit&amp;section=8" title="Edit section: Other">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Behnke relied only on the sign of the gradient (<a href="/wiki/Rprop" title="Rprop">Rprop</a>) when training his <a href="/w/index.php?title=Neural_Abstraction_Pyramid&amp;action=edit&amp;redlink=1" class="new" title="Neural Abstraction Pyramid (page does not exist)">Neural Abstraction Pyramid</a><sup id="cite_ref-18" class="reference"><a href="#cite_note-18">&#91;18&#93;</a></sup> to solve problems like image reconstruction and face localization.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (June 2017)">citation needed</span></a></i>&#93;</sup>
</p><p>Neural networks can also be optimized by using a universal search algorithm on the space of neural network's weights, e.g.,  <a href="/wiki/Random_guess" class="mw-redirect" title="Random guess">random guess</a> or more systematically <a href="/wiki/Genetic_algorithm" title="Genetic algorithm">genetic algorithm</a>. This approach is not based on gradient and avoids the vanishing gradient problem.<sup id="cite_ref-19" class="reference"><a href="#cite_note-19">&#91;19&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Vanishing_gradient_problem&amp;action=edit&amp;section=9" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="/wiki/Spectral_radius" title="Spectral radius">Spectral radius</a></li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Vanishing_gradient_problem&amp;action=edit&amp;section=10" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist columns references-column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;">
<ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><a href="/wiki/Sepp_Hochreiter" title="Sepp Hochreiter">S. Hochreiter</a>. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut f. Informatik, Technische Univ. Munich, 1991. Advisor: <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">J. Schmidhuber</a>.</span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><a href="/wiki/Sepp_Hochreiter" title="Sepp Hochreiter">S. Hochreiter</a>, Y. Bengio, P. Frasconi, and <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">J. Schmidhuber</a>. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. In S. C. Kremer and J. F. Kolen, editors, A Field Guide to Dynamical Recurrent Neural Networks. IEEE Press, 2001.</span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><cite class="citation journal">Goh, Garrett B.; Hodas, Nathan O.; Vishnu, Abhinav (2017-06-15). <a rel="nofollow" class="external text" href="http://onlinelibrary.wiley.com/doi/10.1002/jcc.24764/abstract">"Deep learning for computational chemistry"</a>. <i>Journal of Computational Chemistry</i>. <b>38</b> (16): 1291–1307. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1701.04503">1701.04503</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1002%2Fjcc.24764">10.1002/jcc.24764</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/28272810">28272810</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Computational+Chemistry&amp;rft.atitle=Deep+learning+for+computational+chemistry&amp;rft.volume=38&amp;rft.issue=16&amp;rft.pages=1291-1307&amp;rft.date=2017-06-15&amp;rft_id=info%3Aarxiv%2F1701.04503&amp;rft_id=info%3Apmid%2F28272810&amp;rft_id=info%3Adoi%2F10.1002%2Fjcc.24764&amp;rft.aulast=Goh&amp;rft.aufirst=Garrett+B.&amp;rft.au=Hodas%2C+Nathan+O.&amp;rft.au=Vishnu%2C+Abhinav&amp;rft_id=http%3A%2F%2Fonlinelibrary.wiley.com%2Fdoi%2F10.1002%2Fjcc.24764%2Fabstract&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVanishing+gradient+problem" class="Z3988"></span><style data-mw-deduplicate="TemplateStyles:r861714446">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output q{quotes:"\"""\"""'""'"}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-lock-limited a,.mw-parser-output .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Pascanu, Razvan; Mikolov, Tomas; Bengio, Yoshua (2012-11-21). "On the difficulty of training Recurrent Neural Networks". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1211.5063">1211.5063</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.LG">cs.LG</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=On+the+difficulty+of+training+Recurrent+Neural+Networks&amp;rft.date=2012-11-21&amp;rft_id=info%3Aarxiv%2F1211.5063&amp;rft.aulast=Pascanu&amp;rft.aufirst=Razvan&amp;rft.au=Mikolov%2C+Tomas&amp;rft.au=Bengio%2C+Yoshua&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVanishing+gradient+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r861714446"/></span>
</li>
<li id="cite_note-SCHMID1992-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-SCHMID1992_5-0">^</a></b></span> <span class="reference-text">J. Schmidhuber., "Learning complex, extended sequences using the principle of history compression," <i>Neural Computation</i>, 4, pp. 234–242, 1992.</span>
</li>
<li id="cite_note-hinton2006-6"><span class="mw-cite-backlink">^ <a href="#cite_ref-hinton2006_6-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-hinton2006_6-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal"><a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Hinton, G. E.</a>; Osindero, S.; Teh, Y. (2006). <a rel="nofollow" class="external text" href="http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf">"A fast learning algorithm for deep belief nets"</a> <span class="cs1-format">(PDF)</span>. <i><a href="/wiki/Neural_Computation_(journal)" title="Neural Computation (journal)">Neural Computation</a></i>. <b>18</b> (7): 1527–1554. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1162%2Fneco.2006.18.7.1527">10.1162/neco.2006.18.7.1527</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/16764513">16764513</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computation&amp;rft.atitle=A+fast+learning+algorithm+for+deep+belief+nets&amp;rft.volume=18&amp;rft.issue=7&amp;rft.pages=1527-1554&amp;rft.date=2006&amp;rft_id=info%3Adoi%2F10.1162%2Fneco.2006.18.7.1527&amp;rft_id=info%3Apmid%2F16764513&amp;rft.aulast=Hinton&amp;rft.aufirst=G.+E.&amp;rft.au=Osindero%2C+S.&amp;rft.au=Teh%2C+Y.&amp;rft_id=http%3A%2F%2Fwww.cs.toronto.edu%2F~hinton%2Fabsps%2Ffastnc.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVanishing+gradient+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r861714446"/></span>
</li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text"><cite class="citation journal">Hinton, G. (2009). <a rel="nofollow" class="external text" href="http://www.scholarpedia.org/article/Deep_belief_networks">"Deep belief networks"</a>. <i>Scholarpedia</i>. <b>4</b> (5): 5947. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="http://adsabs.harvard.edu/abs/2009SchpJ...4.5947H">2009SchpJ...4.5947H</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.4249%2Fscholarpedia.5947">10.4249/scholarpedia.5947</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Scholarpedia&amp;rft.atitle=Deep+belief+networks&amp;rft.volume=4&amp;rft.issue=5&amp;rft.pages=5947&amp;rft.date=2009&amp;rft_id=info%3Adoi%2F10.4249%2Fscholarpedia.5947&amp;rft_id=info%3Abibcode%2F2009SchpJ...4.5947H&amp;rft.aulast=Hinton&amp;rft.aufirst=G.&amp;rft_id=http%3A%2F%2Fwww.scholarpedia.org%2Farticle%2FDeep_belief_networks&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVanishing+gradient+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r861714446"/></span>
</li>
<li id="cite_note-lstm-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-lstm_8-0">^</a></b></span> <span class="reference-text"><cite class="citation journal"><a href="/wiki/Sepp_Hochreiter" title="Sepp Hochreiter">Hochreiter, Sepp</a>; <a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Schmidhuber, Jürgen</a> (1997). "Long Short-Term Memory". <i>Neural Computation</i>. <b>9</b> (8): 1735–1780. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1162%2Fneco.1997.9.8.1735">10.1162/neco.1997.9.8.1735</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/9377276">9377276</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computation&amp;rft.atitle=Long+Short-Term+Memory&amp;rft.volume=9&amp;rft.issue=8&amp;rft.pages=1735-1780&amp;rft.date=1997&amp;rft_id=info%3Adoi%2F10.1162%2Fneco.1997.9.8.1735&amp;rft_id=info%3Apmid%2F9377276&amp;rft.aulast=Hochreiter&amp;rft.aufirst=Sepp&amp;rft.au=Schmidhuber%2C+J%C3%BCrgen&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVanishing+gradient+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r861714446"/></span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text">Graves, Alex; and Schmidhuber, Jürgen; <i>Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks</i>, in Bengio, Yoshua; Schuurmans, Dale; Lafferty, John; Williams, Chris K. I.; and Culotta, Aron (eds.), <i>Advances in Neural Information Processing Systems 22 (NIPS'22), December 7th–10th, 2009, Vancouver, BC</i>, Neural Information Processing Systems (NIPS) Foundation, 2009, pp. 545–552</span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><cite class="citation journal">Graves, A.; Liwicki, M.; Fernandez, S.; Bertolami, R.; Bunke, H.; Schmidhuber, J. (2009). "A Novel Connectionist System for Improved Unconstrained Handwriting Recognition". <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>. <b>31</b> (5): 855–868. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1109%2Ftpami.2008.137">10.1109/tpami.2008.137</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence&amp;rft.atitle=A+Novel+Connectionist+System+for+Improved+Unconstrained+Handwriting+Recognition&amp;rft.volume=31&amp;rft.issue=5&amp;rft.pages=855-868&amp;rft.date=2009&amp;rft_id=info%3Adoi%2F10.1109%2Ftpami.2008.137&amp;rft.aulast=Graves&amp;rft.aufirst=A.&amp;rft.au=Liwicki%2C+M.&amp;rft.au=Fernandez%2C+S.&amp;rft.au=Bertolami%2C+R.&amp;rft.au=Bunke%2C+H.&amp;rft.au=Schmidhuber%2C+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVanishing+gradient+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r861714446"/></span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text"><cite class="citation journal">Schmidhuber, Jürgen (2015). "Deep learning in neural networks: An overview". <i>Neural Networks</i>. <b>61</b>: 85–117. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1404.7828">1404.7828</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1016%2Fj.neunet.2014.09.003">10.1016/j.neunet.2014.09.003</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/25462637">25462637</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Networks&amp;rft.atitle=Deep+learning+in+neural+networks%3A+An+overview&amp;rft.volume=61&amp;rft.pages=85-117&amp;rft.date=2015&amp;rft_id=info%3Aarxiv%2F1404.7828&amp;rft_id=info%3Apmid%2F25462637&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neunet.2014.09.003&amp;rft.aulast=Schmidhuber&amp;rft.aufirst=J%C3%BCrgen&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVanishing+gradient+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r861714446"/></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://blog.init.ai/residual-neural-networks-are-an-exciting-area-of-deep-learning-research-acf14f4912e9">"Residual neural networks are an exciting area of deep learning research"</a>. 28 April 2016.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Residual+neural+networks+are+an+exciting+area+of+deep+learning+research&amp;rft.date=2016-04-28&amp;rft_id=https%3A%2F%2Fblog.init.ai%2Fresidual-neural-networks-are-an-exciting-area-of-deep-learning-research-acf14f4912e9&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVanishing+gradient+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r861714446"/></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external free" href="http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf">http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf</a></span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32">"ResNets, HighwayNets, and DenseNets, Oh My! – Chatbot's Life"</a>. 14 October 2016.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=ResNets%2C+HighwayNets%2C+and+DenseNets%2C+Oh+My%21+%E2%80%93+Chatbot%E2%80%99s+Life&amp;rft.date=2016-10-14&amp;rft_id=https%3A%2F%2Fchatbotslife.com%2Fresnets-highwaynets-and-densenets-oh-my-9bb15918ee32&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVanishing+gradient+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r861714446"/></span>
</li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">He, Kaiming; Zhang, Xiangyu; Ren, Shaoqing; Sun, Jian (2015). "Deep Residual Learning for Image Recognition". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1512.03385">1512.03385</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.CV">cs.CV</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Deep+Residual+Learning+for+Image+Recognition&amp;rft.date=2015&amp;rft_id=info%3Aarxiv%2F1512.03385&amp;rft.aulast=He&amp;rft.aufirst=Kaiming&amp;rft.au=Zhang%2C+Xiangyu&amp;rft.au=Ren%2C+Shaoqing&amp;rft.au=Sun%2C+Jian&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVanishing+gradient+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r861714446"/></span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Veit, Andreas; Wilber, Michael; Belongie, Serge (2016-05-20). "Residual Networks Behave Like Ensembles of Relatively Shallow Networks". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1605.06431">1605.06431</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.CV">cs.CV</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Residual+Networks+Behave+Like+Ensembles+of+Relatively+Shallow+Networks&amp;rft.date=2016-05-20&amp;rft_id=info%3Aarxiv%2F1605.06431&amp;rft.aulast=Veit&amp;rft.aufirst=Andreas&amp;rft.au=Wilber%2C+Michael&amp;rft.au=Belongie%2C+Serge&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVanishing+gradient+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r861714446"/></span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text"><cite class="citation journal">Glorot, Xavier; Bordes, Antoine; Bengio, Yoshua (2011-06-14). <a rel="nofollow" class="external text" href="http://proceedings.mlr.press/v15/glorot11a.html">"Deep Sparse Rectifier Neural Networks"</a>. <i>PMLR</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=PMLR&amp;rft.atitle=Deep+Sparse+Rectifier+Neural+Networks&amp;rft.date=2011-06-14&amp;rft.aulast=Glorot&amp;rft.aufirst=Xavier&amp;rft.au=Bordes%2C+Antoine&amp;rft.au=Bengio%2C+Yoshua&amp;rft_id=http%3A%2F%2Fproceedings.mlr.press%2Fv15%2Fglorot11a.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVanishing+gradient+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r861714446"/></span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text"><cite class="citation book">Sven Behnke (2003). <a rel="nofollow" class="external text" href="http://www.ais.uni-bonn.de/books/LNCS2766.pdf"><i>Hierarchical Neural Networks for Image Interpretation</i></a> <span class="cs1-format">(PDF)</span>. Lecture Notes in Computer Science. <b>2766</b>. Springer.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Hierarchical+Neural+Networks+for+Image+Interpretation.&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pub=Springer&amp;rft.date=2003&amp;rft.au=Sven+Behnke&amp;rft_id=http%3A%2F%2Fwww.ais.uni-bonn.de%2Fbooks%2FLNCS2766.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVanishing+gradient+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r861714446"/></span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://people.idsia.ch/~juergen/fundamentaldeeplearningproblem.html">"Sepp Hochreiter's Fundamental Deep Learning Problem (1991)"</a>. <i>people.idsia.ch</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-01-07</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=people.idsia.ch&amp;rft.atitle=Sepp+Hochreiter%27s+Fundamental+Deep+Learning+Problem+%281991%29&amp;rft_id=http%3A%2F%2Fpeople.idsia.ch%2F~juergen%2Ffundamentaldeeplearningproblem.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVanishing+gradient+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r861714446"/></span>
</li>
</ol></div>

<!-- 
NewPP limit report
Parsed by mw1251
Cached time: 20181026044311
Cache expiry: 1900800
Dynamic content: false
CPU time usage: 0.364 seconds
Real time usage: 0.463 seconds
Preprocessor visited node count: 1496/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 65519/2097152 bytes
Template argument size: 1926/2097152 bytes
Highest expansion depth: 11/40
Expensive parser function count: 6/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 41306/5000000 bytes
Number of Wikibase entities loaded: 3/400
Lua time usage: 0.198/10.000 seconds
Lua memory usage: 6.03 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  380.998      1 -total
 56.56%  215.476      1 Template:Reflist
 36.98%  140.906      7 Template:Cite_journal
 14.44%   54.998      1 Template:Machine_learning_bar
 13.63%   51.949      1 Template:Sidebar_with_collapsible_lists
  8.51%   32.441      1 Template:Refimprove_science
  8.36%   31.868      2 Template:Ambox
  7.02%   26.734      1 Template:Clarify
  6.14%   23.387      1 Template:Fix-span
  6.03%   22.986      3 Template:Cite_arxiv
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:43502368-0!canonical and timestamp 20181026044311 and revision id 863125122
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>					<div class="printfooter">
						Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Vanishing_gradient_problem&amp;oldid=863125122">https://en.wikipedia.org/w/index.php?title=Vanishing_gradient_problem&amp;oldid=863125122</a>"					</div>
				<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li><li><a href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:Articles_lacking_reliable_references_from_December_2017" title="Category:Articles lacking reliable references from December 2017">Articles lacking reliable references from December 2017</a></li><li><a href="/wiki/Category:All_articles_lacking_reliable_references" title="Category:All articles lacking reliable references">All articles lacking reliable references</a></li><li><a href="/wiki/Category:Wikipedia_articles_needing_clarification_from_October_2018" title="Category:Wikipedia articles needing clarification from October 2018">Wikipedia articles needing clarification from October 2018</a></li><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_June_2017" title="Category:Articles with unsourced statements from June 2017">Articles with unsourced statements from June 2017</a></li></ul></div></div>				<div class="visualClear"></div>
							</div>
		</div>
		<div id="mw-navigation">
			<h2>Navigation menu</h2>
			<div id="mw-head">
									<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Vanishing+gradient+problem" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Vanishing+gradient+problem" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
							<li id="ca-nstab-main" class="selected"><span><a href="/wiki/Vanishing_gradient_problem" title="View the content page [c]" accesskey="c">Article</a></span></li><li id="ca-talk"><span><a href="/wiki/Talk:Vanishing_gradient_problem" rel="discussion" title="Discussion about the content page [t]" accesskey="t">Talk</a></span></li>						</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label" />
						<h3 id="p-variants-label">
							<span>Variants</span>
						</h3>
						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
							<li id="ca-view" class="collapsible selected"><span><a href="/wiki/Vanishing_gradient_problem">Read</a></span></li><li id="ca-edit" class="collapsible"><span><a href="/w/index.php?title=Vanishing_gradient_problem&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></span></li><li id="ca-history" class="collapsible"><span><a href="/w/index.php?title=Vanishing_gradient_problem&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></span></li>						</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
						<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" />
						<h3 id="p-cactions-label"><span>More</span></h3>
						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>
						<form action="/w/index.php" id="searchform">
							<div id="simpleSearch">
								<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/><input type="hidden" value="Special:Search" name="title"/><input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/><input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>							</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="/wiki/Main_Page"  title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id="p-navigation" aria-labelledby="p-navigation-label">
			<h3 id="p-navigation-label">Navigation</h3>
			<div class="body">
								<ul>
					<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-interaction" aria-labelledby="p-interaction-label">
			<h3 id="p-interaction-label">Interaction</h3>
			<div class="body">
								<ul>
					<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-tb" aria-labelledby="p-tb-label">
			<h3 id="p-tb-label">Tools</h3>
			<div class="body">
								<ul>
					<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Vanishing_gradient_problem" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Vanishing_gradient_problem" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Vanishing_gradient_problem&amp;oldid=863125122" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Vanishing_gradient_problem&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q18358230" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Vanishing_gradient_problem&amp;id=863125122" title="Information on how to cite this page">Cite this page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-coll-print_export" aria-labelledby="p-coll-print_export-label">
			<h3 id="p-coll-print_export-label">Print/export</h3>
			<div class="body">
								<ul>
					<li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Vanishing+gradient+problem">Create a book</a></li><li id="coll-download-as-rdf2latex"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Vanishing+gradient+problem&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=Vanishing_gradient_problem&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-lang" aria-labelledby="p-lang-label">
			<h3 id="p-lang-label">Languages</h3>
			<div class="body">
								<ul>
					<li class="interlanguage-link interwiki-it"><a href="https://it.wikipedia.org/wiki/Problema_della_scomparsa_del_gradiente" title="Problema della scomparsa del gradiente – Italian" lang="it" hreflang="it" class="interlanguage-link-target">Italiano</a></li><li class="interlanguage-link interwiki-uk"><a href="https://uk.wikipedia.org/wiki/%D0%9F%D1%80%D0%BE%D0%B1%D0%BB%D0%B5%D0%BC%D0%B0_%D0%B7%D0%BD%D0%B8%D0%BA%D0%B0%D0%BD%D0%BD%D1%8F_%D0%B3%D1%80%D0%B0%D0%B4%D1%96%D1%94%D0%BD%D1%82%D1%83" title="Проблема зникання градієнту – Ukrainian" lang="uk" hreflang="uk" class="interlanguage-link-target">Українська</a></li><li class="interlanguage-link interwiki-zh"><a href="https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98" title="梯度消失问题 – Chinese" lang="zh" hreflang="zh" class="interlanguage-link-target">中文</a></li>				</ul>
				<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q18358230#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>			</div>
		</div>
				</div>
		</div>
				<div id="footer" role="contentinfo">
						<ul id="footer-info">
								<li id="footer-info-lastmod"> This page was last edited on 8 October 2018, at 21:30<span class="anonymous-show"> (UTC)</span>.</li>
								<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
							</ul>
						<ul id="footer-places">
								<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
								<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
								<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
								<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
								<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
								<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
								<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Vanishing_gradient_problem&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
							</ul>
										<ul id="footer-icons" class="noprint">
										<li id="footer-copyrightico">
						<a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a>					</li>
										<li id="footer-poweredbyico">
						<a href="//www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a>					</li>
									</ul>
						<div style="clear: both;"></div>
		</div>
		
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.364","walltime":"0.463","ppvisitednodes":{"value":1496,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":65519,"limit":2097152},"templateargumentsize":{"value":1926,"limit":2097152},"expansiondepth":{"value":11,"limit":40},"expensivefunctioncount":{"value":6,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":41306,"limit":5000000},"entityaccesscount":{"value":3,"limit":400},"timingprofile":["100.00%  380.998      1 -total"," 56.56%  215.476      1 Template:Reflist"," 36.98%  140.906      7 Template:Cite_journal"," 14.44%   54.998      1 Template:Machine_learning_bar"," 13.63%   51.949      1 Template:Sidebar_with_collapsible_lists","  8.51%   32.441      1 Template:Refimprove_science","  8.36%   31.868      2 Template:Ambox","  7.02%   26.734      1 Template:Clarify","  6.14%   23.387      1 Template:Fix-span","  6.03%   22.986      3 Template:Cite_arxiv"]},"scribunto":{"limitreport-timeusage":{"value":"0.198","limit":"10.000"},"limitreport-memusage":{"value":6320891,"limit":52428800}},"cachereport":{"origin":"mw1251","timestamp":"20181026044311","ttl":1900800,"transientcontent":false}}});mw.config.set({"wgBackendResponseTime":98,"wgHostname":"mw1272"});});</script>
	</body>
</html>
